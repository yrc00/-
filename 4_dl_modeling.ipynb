{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "5-fold MAE: 12.6128\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1  # L1 정규화 임포트\n",
    "\n",
    "# 데이터 로드\n",
    "train = pd.read_csv('./train.csv')\n",
    "\n",
    "# yymm 컬럼을 날짜 형식으로 변환 (연도는 임의로 설정)\n",
    "train['yymm'] = pd.to_datetime('2024' + train['yymm'], format='%Y%m%d %H:%M')\n",
    "\n",
    "# day, hour, minute 컬럼 생성\n",
    "train['day'] = train['yymm'].dt.day         # 일\n",
    "train['hour'] = train['yymm'].dt.hour       # 시\n",
    "train['minute'] = train['yymm'].dt.minute   # 분\n",
    "\n",
    "# weekday 컬럼 생성\n",
    "train['weekday'] = train['day'] % 7         # 요일 (0: 월요일, 1: 화요일, ..., 6: 일요일)\n",
    "\n",
    "# yymm 컬럼 삭제\n",
    "train.drop('yymm', axis=1, inplace=True)\n",
    "\n",
    "# 데이터 분할\n",
    "X = train.drop('Target', axis=1)    # Target을 제외한 모든 컬럼을 X로 지정\n",
    "y = train['Target']                 # Target 컬럼을 y로 지정\n",
    "\n",
    "# 특징 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# K-fold cross-validation 설정 (5-fold)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "mae_scores = []\n",
    "\n",
    "# 딥러닝 회귀 모델 정의 함수\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_scaled.shape[1],)),  # Input layer로 input shape 지정\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1(0.01)),  # L1 규제 추가\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l1(0.01)),  # L1 규제 추가\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1)  # 회귀 모델이므로 활성화 함수 없이 출력\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
    "    return model\n",
    "\n",
    "# 5-fold 교차 검증\n",
    "for train_index, val_index in kf.split(X_scaled):\n",
    "    X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # 모델 생성 및 학습\n",
    "    model = create_model()\n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val), verbose=0)\n",
    "\n",
    "    # 예측 및 MAE 계산\n",
    "    y_pred = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    mae_scores.append(mae)\n",
    "\n",
    "# 5-fold MAE 평균 출력\n",
    "print(f'5-fold MAE: {np.mean(mae_scores):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.001, Batch Size: 16, 5-fold MAE: 12.6073\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Learning Rate: 0.001, Batch Size: 32, 5-fold MAE: 12.5236\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Learning Rate: 0.001, Batch Size: 64, 5-fold MAE: 12.6161\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Learning Rate: 0.01, Batch Size: 16, 5-fold MAE: 12.5492\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "Learning Rate: 0.01, Batch Size: 32, 5-fold MAE: 12.5349\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Learning Rate: 0.01, Batch Size: 64, 5-fold MAE: 12.5384\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Learning Rate: 0.1, Batch Size: 16, 5-fold MAE: 12.5472\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Learning Rate: 0.1, Batch Size: 32, 5-fold MAE: 12.5302\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Learning Rate: 0.1, Batch Size: 64, 5-fold MAE: 12.5382\n",
      "\n",
      "Results:\n",
      "Learning Rate: 0.001, Batch Size: 16, Mean MAE: 12.6073\n",
      "Learning Rate: 0.001, Batch Size: 32, Mean MAE: 12.5236\n",
      "Learning Rate: 0.001, Batch Size: 64, Mean MAE: 12.6161\n",
      "Learning Rate: 0.01, Batch Size: 16, Mean MAE: 12.5492\n",
      "Learning Rate: 0.01, Batch Size: 32, Mean MAE: 12.5349\n",
      "Learning Rate: 0.01, Batch Size: 64, Mean MAE: 12.5384\n",
      "Learning Rate: 0.1, Batch Size: 16, Mean MAE: 12.5472\n",
      "Learning Rate: 0.1, Batch Size: 32, Mean MAE: 12.5302\n",
      "Learning Rate: 0.1, Batch Size: 64, Mean MAE: 12.5382\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1  # L1 정규화 임포트\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "np.random.seed(42)  # NumPy 랜덤 시드\n",
    "tf.random.set_seed(42)  # TensorFlow 랜덤 시드\n",
    "\n",
    "# 데이터 로드\n",
    "train = pd.read_csv('./train.csv')\n",
    "\n",
    "# yymm 컬럼을 날짜 형식으로 변환 (연도는 임의로 설정)\n",
    "train['yymm'] = pd.to_datetime('2024' + train['yymm'], format='%Y%m%d %H:%M')\n",
    "\n",
    "# day, hour, minute 컬럼 생성\n",
    "train['day'] = train['yymm'].dt.day         # 일\n",
    "train['hour'] = train['yymm'].dt.hour       # 시\n",
    "train['minute'] = train['yymm'].dt.minute   # 분\n",
    "\n",
    "# weekday 컬럼 생성\n",
    "train['weekday'] = train['day'] % 7         # 요일 (0: 월요일, 1: 화요일, ..., 6: 일요일)\n",
    "\n",
    "# yymm 컬럼 삭제\n",
    "train.drop('yymm', axis=1, inplace=True)\n",
    "\n",
    "# 데이터 분할\n",
    "X = train.drop('Target', axis=1)    # Target을 제외한 모든 컬럼을 X로 지정\n",
    "y = train['Target']                 # Target 컬럼을 y로 지정\n",
    "\n",
    "# 특징 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# K-fold cross-validation 설정 (5-fold)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 설정한 하이퍼파라미터\n",
    "learning_rates = [0.001, 0.01, 0.1]  # 여러 learning rate\n",
    "batch_sizes = [16, 32, 64]       # 여러 batch size\n",
    "\n",
    "# 성능 비교를 위한 결과 저장\n",
    "results = []\n",
    "\n",
    "# 딥러닝 회귀 모델 정의 함수 (DNN)\n",
    "def create_model(learning_rate):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_scaled.shape[1],)),  # Input layer로 input shape 지정\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1(0.01)),  # DNN 레이어 (크기 축소)\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l1(0.01)),   # DNN 레이어 (크기 축소)\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1)  # 회귀 모델이므로 활성화 함수 없이 출력\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
    "    return model\n",
    "\n",
    "# 다양한 learning rate와 batch size에 대해 성능 비교\n",
    "for learning_rate in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        mae_scores = []\n",
    "\n",
    "        # 5-fold 교차 검증\n",
    "        for train_index, val_index in kf.split(X_scaled):\n",
    "            X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "            # 모델 생성 및 학습\n",
    "            model = create_model(learning_rate)\n",
    "\n",
    "            # EarlyStopping 설정\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "            model.fit(X_train, y_train, epochs=100, batch_size=batch_size, validation_data=(X_val, y_val),verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "            # 예측 및 MAE 계산\n",
    "            y_pred = model.predict(X_val)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            mae_scores.append(mae)\n",
    "\n",
    "        # 평균 MAE 결과 저장\n",
    "        mean_mae = np.mean(mae_scores)\n",
    "        results.append((learning_rate, batch_size, mean_mae))\n",
    "        print(f'Learning Rate: {learning_rate}, Batch Size: {batch_size}, 5-fold MAE: {mean_mae:.4f}')\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\nResults:\")\n",
    "for lr, bs, mae in results:\n",
    "    print(f'Learning Rate: {lr}, Batch Size: {bs}, Mean MAE: {mae:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.001, Batch Size: 16, 5-fold MAE: 12.6073\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Learning Rate: 0.001, Batch Size: 32, 5-fold MAE: 12.5236\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Learning Rate: 0.001, Batch Size: 64, 5-fold MAE: 12.6161\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Learning Rate: 0.01, Batch Size: 16, 5-fold MAE: 12.5492\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "Learning Rate: 0.01, Batch Size: 32, 5-fold MAE: 12.5349\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Learning Rate: 0.01, Batch Size: 64, 5-fold MAE: 12.5384\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Learning Rate: 0.1, Batch Size: 16, 5-fold MAE: 12.5472\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Learning Rate: 0.1, Batch Size: 32, 5-fold MAE: 12.5302\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Learning Rate: 0.1, Batch Size: 64, 5-fold MAE: 12.5382\n",
      "\n",
      "Results:\n",
      "Learning Rate: 0.001, Batch Size: 16, Mean MAE: 12.6073\n",
      "Learning Rate: 0.001, Batch Size: 32, Mean MAE: 12.5236\n",
      "Learning Rate: 0.001, Batch Size: 64, Mean MAE: 12.6161\n",
      "Learning Rate: 0.01, Batch Size: 16, Mean MAE: 12.5492\n",
      "Learning Rate: 0.01, Batch Size: 32, Mean MAE: 12.5349\n",
      "Learning Rate: 0.01, Batch Size: 64, Mean MAE: 12.5384\n",
      "Learning Rate: 0.1, Batch Size: 16, Mean MAE: 12.5472\n",
      "Learning Rate: 0.1, Batch Size: 32, Mean MAE: 12.5302\n",
      "Learning Rate: 0.1, Batch Size: 64, Mean MAE: 12.5382\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1  # L1 정규화 임포트\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "np.random.seed(42)  # NumPy 랜덤 시드\n",
    "tf.random.set_seed(42)  # TensorFlow 랜덤 시드\n",
    "\n",
    "# 데이터 로드\n",
    "train = pd.read_csv('./train.csv')\n",
    "\n",
    "# yymm 컬럼을 날짜 형식으로 변환 (연도는 임의로 설정)\n",
    "train['yymm'] = pd.to_datetime('2024' + train['yymm'], format='%Y%m%d %H:%M')\n",
    "\n",
    "# day, hour, minute 컬럼 생성\n",
    "train['day'] = train['yymm'].dt.day         # 일\n",
    "train['hour'] = train['yymm'].dt.hour       # 시\n",
    "train['minute'] = train['yymm'].dt.minute   # 분\n",
    "\n",
    "# weekday 컬럼 생성\n",
    "train['weekday'] = train['day'] % 7         # 요일 (0: 월요일, 1: 화요일, ..., 6: 일요일)\n",
    "\n",
    "# yymm 컬럼 삭제\n",
    "train.drop('yymm', axis=1, inplace=True)\n",
    "\n",
    "# 데이터 분할\n",
    "X = train.drop('Target', axis=1)    # Target을 제외한 모든 컬럼을 X로 지정\n",
    "y = train['Target']                 # Target 컬럼을 y로 지정\n",
    "\n",
    "# 특징 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# K-fold cross-validation 설정 (5-fold)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 설정한 하이퍼파라미터\n",
    "learning_rates = [0.001, 0.01, 0.1]  # 여러 learning rate\n",
    "batch_sizes = [16, 32, 64]       # 여러 batch size\n",
    "\n",
    "# 성능 비교를 위한 결과 저장\n",
    "results = []\n",
    "\n",
    "# 딥러닝 회귀 모델 정의 함수 (DNN)\n",
    "def create_model(learning_rate):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_scaled.shape[1],)),  # Input layer로 input shape 지정\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1(0.01)),  # DNN 레이어 (크기 축소)\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l1(0.01)),   # DNN 레이어 (크기 축소)\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1)  # 회귀 모델이므로 활성화 함수 없이 출력\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
    "    return model\n",
    "\n",
    "# 다양한 learning rate와 batch size에 대해 성능 비교\n",
    "for learning_rate in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        mae_scores = []\n",
    "\n",
    "        # 5-fold 교차 검증\n",
    "        for train_index, val_index in kf.split(X_scaled):\n",
    "            X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "            # 모델 생성 및 학습\n",
    "            model = create_model(learning_rate)\n",
    "\n",
    "            # EarlyStopping 설정\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "            model.fit(X_train, y_train, epochs=100, batch_size=batch_size, validation_data=(X_val, y_val),verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "            # 예측 및 MAE 계산\n",
    "            y_pred = model.predict(X_val)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            mae_scores.append(mae)\n",
    "\n",
    "        # 평균 MAE 결과 저장\n",
    "        mean_mae = np.mean(mae_scores)\n",
    "        results.append((learning_rate, batch_size, mean_mae))\n",
    "        print(f'Learning Rate: {learning_rate}, Batch Size: {batch_size}, 5-fold MAE: {mean_mae:.4f}')\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\nResults:\")\n",
    "for lr, bs, mae in results:\n",
    "    print(f'Learning Rate: {lr}, Batch Size: {bs}, Mean MAE: {mae:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.001, Batch Size: 16, 5-fold MAE: 12.6073\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Learning Rate: 0.001, Batch Size: 32, 5-fold MAE: 12.5236\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Learning Rate: 0.001, Batch Size: 64, 5-fold MAE: 12.6161\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Learning Rate: 0.01, Batch Size: 16, 5-fold MAE: 12.5492\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "Learning Rate: 0.01, Batch Size: 32, 5-fold MAE: 12.5349\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Learning Rate: 0.01, Batch Size: 64, 5-fold MAE: 12.5384\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Learning Rate: 0.1, Batch Size: 16, 5-fold MAE: 12.5472\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Learning Rate: 0.1, Batch Size: 32, 5-fold MAE: 12.5302\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Learning Rate: 0.1, Batch Size: 64, 5-fold MAE: 12.5382\n",
      "\n",
      "Results:\n",
      "Learning Rate: 0.001, Batch Size: 16, Mean MAE: 12.6073\n",
      "Learning Rate: 0.001, Batch Size: 32, Mean MAE: 12.5236\n",
      "Learning Rate: 0.001, Batch Size: 64, Mean MAE: 12.6161\n",
      "Learning Rate: 0.01, Batch Size: 16, Mean MAE: 12.5492\n",
      "Learning Rate: 0.01, Batch Size: 32, Mean MAE: 12.5349\n",
      "Learning Rate: 0.01, Batch Size: 64, Mean MAE: 12.5384\n",
      "Learning Rate: 0.1, Batch Size: 16, Mean MAE: 12.5472\n",
      "Learning Rate: 0.1, Batch Size: 32, Mean MAE: 12.5302\n",
      "Learning Rate: 0.1, Batch Size: 64, Mean MAE: 12.5382\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1  # L1 정규화 임포트\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "np.random.seed(42)  # NumPy 랜덤 시드\n",
    "tf.random.set_seed(42)  # TensorFlow 랜덤 시드\n",
    "\n",
    "# 데이터 로드\n",
    "train = pd.read_csv('./train.csv')\n",
    "\n",
    "# yymm 컬럼을 날짜 형식으로 변환 (연도는 임의로 설정)\n",
    "train['yymm'] = pd.to_datetime('2024' + train['yymm'], format='%Y%m%d %H:%M')\n",
    "\n",
    "# day, hour, minute 컬럼 생성\n",
    "train['day'] = train['yymm'].dt.day         # 일\n",
    "train['hour'] = train['yymm'].dt.hour       # 시\n",
    "train['minute'] = train['yymm'].dt.minute   # 분\n",
    "\n",
    "# weekday 컬럼 생성\n",
    "train['weekday'] = train['day'] % 7         # 요일 (0: 월요일, 1: 화요일, ..., 6: 일요일)\n",
    "\n",
    "# yymm 컬럼 삭제\n",
    "train.drop('yymm', axis=1, inplace=True)\n",
    "\n",
    "# 데이터 분할\n",
    "X = train.drop('Target', axis=1)    # Target을 제외한 모든 컬럼을 X로 지정\n",
    "y = train['Target']                 # Target 컬럼을 y로 지정\n",
    "\n",
    "# 특징 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# K-fold cross-validation 설정 (5-fold)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 설정한 하이퍼파라미터\n",
    "learning_rates = [0.001, 0.01, 0.1]  # 여러 learning rate\n",
    "batch_sizes = [16, 32, 64]       # 여러 batch size\n",
    "\n",
    "# 성능 비교를 위한 결과 저장\n",
    "results = []\n",
    "\n",
    "# 딥러닝 회귀 모델 정의 함수 (DNN)\n",
    "def create_model(learning_rate):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_scaled.shape[1],)),  # Input layer로 input shape 지정\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1(0.01)),  # DNN 레이어 (크기 축소)\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l1(0.01)),   # DNN 레이어 (크기 축소)\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1)  # 회귀 모델이므로 활성화 함수 없이 출력\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
    "    return model\n",
    "\n",
    "# 다양한 learning rate와 batch size에 대해 성능 비교\n",
    "for learning_rate in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        mae_scores = []\n",
    "\n",
    "        # 5-fold 교차 검증\n",
    "        for train_index, val_index in kf.split(X_scaled):\n",
    "            X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "            # 모델 생성 및 학습\n",
    "            model = create_model(learning_rate)\n",
    "\n",
    "            # EarlyStopping 설정\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "            model.fit(X_train, y_train, epochs=100, batch_size=batch_size, validation_data=(X_val, y_val),verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "            # 예측 및 MAE 계산\n",
    "            y_pred = model.predict(X_val)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            mae_scores.append(mae)\n",
    "\n",
    "        # 평균 MAE 결과 저장\n",
    "        mean_mae = np.mean(mae_scores)\n",
    "        results.append((learning_rate, batch_size, mean_mae))\n",
    "        print(f'Learning Rate: {learning_rate}, Batch Size: {batch_size}, 5-fold MAE: {mean_mae:.4f}')\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\nResults:\")\n",
    "for lr, bs, mae in results:\n",
    "    print(f'Learning Rate: {lr}, Batch Size: {bs}, Mean MAE: {mae:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "# Fix random seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('./train.csv')\n",
    "# Assuming test dataset does not include target column\n",
    "test = pd.read_csv('./test_set.csv')\n",
    "\n",
    "# Convert 'yymm' to datetime format (adding an arbitrary year)\n",
    "train['yymm'] = pd.to_datetime('2024' + train['yymm'], format='%Y%m%d %H:%M')\n",
    "test['yymm'] = pd.to_datetime('2024' + test['yymm'], format='%Y%m%d %H:%M')\n",
    "\n",
    "# Extract day, hour, minute features\n",
    "train['day'] = train['yymm'].dt.day\n",
    "train['hour'] = train['yymm'].dt.hour\n",
    "train['minute'] = train['yymm'].dt.minute\n",
    "test['day'] = test['yymm'].dt.day\n",
    "test['hour'] = test['yymm'].dt.hour\n",
    "test['minute'] = test['yymm'].dt.minute\n",
    "\n",
    "# Generate weekday feature\n",
    "train['weekday'] = train['day'] % 7\n",
    "test['weekday'] = test['day'] % 7\n",
    "\n",
    "# Drop 'yymm' column\n",
    "train.drop('yymm', axis=1, inplace=True)\n",
    "test.drop('yymm', axis=1, inplace=True)\n",
    "\n",
    "# Split into features and target\n",
    "X = train.drop('Target', axis=1)\n",
    "y = train['Target']\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 딥러닝 회귀 모델 정의 함수 (DNN)\n",
    "def create_model(learning_rate, l1_value):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_scaled.shape[1],)),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1(float(l1_value))),  # l1을 float 타입으로 변환\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l1(float(l1_value))),  # l1을 float 타입으로 변환\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
    "    return model\n",
    "\n",
    "# Hyperparameters and L1 values to test\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "l1_values = [0, 0.001, 0.01, 0.1, 1.0, 10.0]  # Different L1 values\n",
    "results = []\n",
    "\n",
    "# Train models with different L1 values and collect MAE\n",
    "for l1_value in l1_values:\n",
    "    model = create_model(learning_rate, l1_value)\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_scaled, y, epochs=100, batch_size=batch_size, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # Collect the final training loss (MAE) for this l1_value\n",
    "    final_mae = history.history['loss'][-1]\n",
    "    results.append((l1_value, final_mae))\n",
    "\n",
    "# Plotting the results\n",
    "l1_values, mae_values = zip(*results)\n",
    "plt.plot(l1_values, mae_values, marker='o')\n",
    "plt.xlabel('L1 Regularization Value')\n",
    "plt.ylabel('Mean Absolute Error (MAE)')\n",
    "plt.title('Effect of L1 Regularization on Model Performance')\n",
    "plt.xscale('log')  # Log scale for better visualization\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 29.4766\n",
      "Epoch 2/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 26.5327\n",
      "Epoch 3/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 22.9374\n",
      "Epoch 4/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 18.9881\n",
      "Epoch 5/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 16.4801\n",
      "Epoch 6/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 15.2468\n",
      "Epoch 7/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14.7318\n",
      "Epoch 8/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14.5219\n",
      "Epoch 9/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14.4723\n",
      "Epoch 10/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14.0884\n",
      "Epoch 11/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14.2285\n",
      "Epoch 12/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 14.0781\n",
      "Epoch 13/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 13.9726\n",
      "Epoch 14/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 13.9564\n",
      "Epoch 15/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.0386\n",
      "Epoch 16/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.8076\n",
      "Epoch 17/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.7959\n",
      "Epoch 18/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.7694\n",
      "Epoch 19/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.8613\n",
      "Epoch 20/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.7518\n",
      "Epoch 21/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 13.6897\n",
      "Epoch 22/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 13.5893\n",
      "Epoch 23/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 13.5908\n",
      "Epoch 24/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 13.5201\n",
      "Epoch 25/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.4961\n",
      "Epoch 26/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.6512\n",
      "Epoch 27/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.5502\n",
      "Epoch 28/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.3626\n",
      "Epoch 29/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.5108\n",
      "Epoch 30/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 13.4180\n",
      "Epoch 31/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 13.3949\n",
      "Epoch 32/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 13.3539\n",
      "Epoch 33/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.3860\n",
      "Epoch 34/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 13.3711\n",
      "Epoch 35/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 13.4298\n",
      "Epoch 36/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 13.4240\n",
      "Epoch 37/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 13.3419\n",
      "Epoch 38/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.3752\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "     predict\n",
      "0  22.437754\n",
      "1  25.711874\n",
      "2  19.357069\n",
      "3  21.321245\n",
      "4  24.958784\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1  # L1 regularization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "# Fix random seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test_set.csv')\n",
    "\n",
    "# Convert 'yymm' to datetime format (adding an arbitrary year)\n",
    "train['yymm'] = pd.to_datetime('2024' + train['yymm'], format='%Y%m%d %H:%M')\n",
    "test['yymm'] = pd.to_datetime('2024' + test['yymm'], format='%Y%m%d %H:%M')\n",
    "\n",
    "# Extract day, hour, minute features\n",
    "train['day'] = train['yymm'].dt.day\n",
    "train['hour'] = train['yymm'].dt.hour\n",
    "train['minute'] = train['yymm'].dt.minute\n",
    "\n",
    "test['day'] = test['yymm'].dt.day\n",
    "test['hour'] = test['yymm'].dt.hour\n",
    "test['minute'] = test['yymm'].dt.minute\n",
    "\n",
    "# Generate weekday feature\n",
    "train['weekday'] = train['day'] % 7\n",
    "test['weekday'] = test['day'] % 7\n",
    "\n",
    "# Drop 'yymm' column\n",
    "train.drop('yymm', axis=1, inplace=True)\n",
    "test.drop('yymm', axis=1, inplace=True)\n",
    "\n",
    "# Split into features and target\n",
    "columns = ['V2', 'V14', 'V20']\n",
    "X = train.drop('Target', axis=1)\n",
    "y = train['Target']\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(test)\n",
    "\n",
    "# 딥러닝 회귀 모델 정의 함수 (DNN)\n",
    "def create_model(learning_rate):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_scaled.shape[1],)),  # Input layer로 input shape 지정\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1(0.01)),  # DNN 레이어 (크기 축소)\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l1(0.01)),   # DNN 레이어 (크기 축소)\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1)  # 회귀 모델이므로 활성화 함수 없이 출력\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
    "    return model\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "\n",
    "# Create the model\n",
    "model = create_model(learning_rate)\n",
    "\n",
    "# Set up EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model on the entire dataset without validation split\n",
    "model.fit(X_scaled, y, epochs=100, batch_size=batch_size, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Generate submission file\n",
    "submission = pd.DataFrame(test_pred, columns=['predict'])\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "5-fold MAE: 12.6021\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1  # L1 정규화 임포트\n",
    "\n",
    "# 데이터 로드\n",
    "train = pd.read_csv('./train.csv')\n",
    "\n",
    "# yymm 컬럼을 날짜 형식으로 변환 (연도는 임의로 설정)\n",
    "train['yymm'] = pd.to_datetime('2024' + train['yymm'], format='%Y%m%d %H:%M')\n",
    "\n",
    "# day, hour, minute 컬럼 생성\n",
    "train['day'] = train['yymm'].dt.day         # 일\n",
    "train['hour'] = train['yymm'].dt.hour       # 시\n",
    "train['minute'] = train['yymm'].dt.minute   # 분\n",
    "\n",
    "# weekday 컬럼 생성\n",
    "train['weekday'] = train['day'] % 7         # 요일 (0: 월요일, 1: 화요일, ..., 6: 일요일)\n",
    "\n",
    "# yymm 컬럼 삭제\n",
    "train.drop('yymm', axis=1, inplace=True)\n",
    "\n",
    "# 데이터 분할\n",
    "X = train.drop('Target', axis=1)    # Target을 제외한 모든 컬럼을 X로 지정\n",
    "y = train['Target']                 # Target 컬럼을 y로 지정\n",
    "\n",
    "# 특징 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# K-fold cross-validation 설정 (5-fold)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "mae_scores = []\n",
    "\n",
    "# 딥러닝 회귀 모델 정의 함수 (DNN)\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_scaled.shape[1],)),  # Input layer로 input shape 지정\n",
    "        Dense(128, activation='relu', kernel_regularizer=l1(0.01)),  # DNN 레이어: 뉴런 수 증가\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1(0.01)),   # 뉴런 수 증가\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l1(0.01)),   # 뉴런 수 증가\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1)  # 회귀 모델이므로 활성화 함수 없이 출력\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
    "    return model\n",
    "\n",
    "# 5-fold 교차 검증\n",
    "for train_index, val_index in kf.split(X_scaled):\n",
    "    X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # 모델 생성 및 학습\n",
    "    model = create_model()\n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val), verbose=0)\n",
    "\n",
    "    # 예측 및 MAE 계산\n",
    "    y_pred = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    mae_scores.append(mae)\n",
    "\n",
    "# 5-fold MAE 평균 출력\n",
    "print(f'5-fold MAE: {np.mean(mae_scores):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.001, Batch Size: 16, 5-fold MAE: 12.5329\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Learning Rate: 0.001, Batch Size: 32, 5-fold MAE: 12.5277\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.001, Batch Size: 64, 5-fold MAE: 12.5826\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.01, Batch Size: 16, 5-fold MAE: 12.5427\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.01, Batch Size: 32, 5-fold MAE: 12.5397\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.01, Batch Size: 64, 5-fold MAE: 12.5429\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Learning Rate: 0.1, Batch Size: 16, 5-fold MAE: 12.5537\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Learning Rate: 0.1, Batch Size: 32, 5-fold MAE: 12.5401\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.1, Batch Size: 64, 5-fold MAE: 12.5728\n",
      "\n",
      "Results:\n",
      "Learning Rate: 0.001, Batch Size: 16, Mean MAE: 12.5329\n",
      "Learning Rate: 0.001, Batch Size: 32, Mean MAE: 12.5277\n",
      "Learning Rate: 0.001, Batch Size: 64, Mean MAE: 12.5826\n",
      "Learning Rate: 0.01, Batch Size: 16, Mean MAE: 12.5427\n",
      "Learning Rate: 0.01, Batch Size: 32, Mean MAE: 12.5397\n",
      "Learning Rate: 0.01, Batch Size: 64, Mean MAE: 12.5429\n",
      "Learning Rate: 0.1, Batch Size: 16, Mean MAE: 12.5537\n",
      "Learning Rate: 0.1, Batch Size: 32, Mean MAE: 12.5401\n",
      "Learning Rate: 0.1, Batch Size: 64, Mean MAE: 12.5728\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1  # L1 정규화 임포트\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "np.random.seed(42)  # NumPy 랜덤 시드\n",
    "tf.random.set_seed(42)  # TensorFlow 랜덤 시드\n",
    "\n",
    "# 데이터 로드\n",
    "train = pd.read_csv('./train.csv')\n",
    "\n",
    "# yymm 컬럼을 날짜 형식으로 변환 (연도는 임의로 설정)\n",
    "train['yymm'] = pd.to_datetime('2024' + train['yymm'], format='%Y%m%d %H:%M')\n",
    "\n",
    "# day, hour, minute 컬럼 생성\n",
    "train['day'] = train['yymm'].dt.day         # 일\n",
    "train['hour'] = train['yymm'].dt.hour       # 시\n",
    "train['minute'] = train['yymm'].dt.minute   # 분\n",
    "\n",
    "# weekday 컬럼 생성\n",
    "train['weekday'] = train['day'] % 7         # 요일 (0: 월요일, 1: 화요일, ..., 6: 일요일)\n",
    "\n",
    "# yymm 컬럼 삭제\n",
    "train.drop('yymm', axis=1, inplace=True)\n",
    "\n",
    "# 데이터 분할\n",
    "X = train.drop('Target', axis=1)    # Target을 제외한 모든 컬럼을 X로 지정\n",
    "y = train['Target']                 # Target 컬럼을 y로 지정\n",
    "\n",
    "# 특징 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# K-fold cross-validation 설정 (5-fold)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 설정한 하이퍼파라미터\n",
    "learning_rates = [0.001, 0.01, 0.1]  # 여러 learning rate\n",
    "batch_sizes = [16, 32, 64]       # 여러 batch size\n",
    "\n",
    "# 성능 비교를 위한 결과 저장\n",
    "results = []\n",
    "\n",
    "# 딥러닝 회귀 모델 정의 함수 (DNN)\n",
    "def create_model(learning_rate):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_scaled.shape[1],)),  # Input layer로 input shape 지정\n",
    "        Dense(128, activation='relu', kernel_regularizer=l1(0.01)),  # DNN 레이어\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1(0.01)),   # DNN 레이어\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l1(0.01)),   # DNN 레이어\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1)  # 회귀 모델이므로 활성화 함수 없이 출력\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
    "    return model\n",
    "\n",
    "# 다양한 learning rate와 batch size에 대해 성능 비교\n",
    "for learning_rate in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        mae_scores = []\n",
    "\n",
    "        # 5-fold 교차 검증\n",
    "        for train_index, val_index in kf.split(X_scaled):\n",
    "            X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "            # 모델 생성 및 학습\n",
    "            model = create_model(learning_rate)\n",
    "\n",
    "            # EarlyStopping 설정\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "            model.fit(X_train, y_train, epochs=100, batch_size=batch_size, validation_data=(X_val, y_val),verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "            # 예측 및 MAE 계산\n",
    "            y_pred = model.predict(X_val)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            mae_scores.append(mae)\n",
    "\n",
    "        # 평균 MAE 결과 저장\n",
    "        mean_mae = np.mean(mae_scores)\n",
    "        results.append((learning_rate, batch_size, mean_mae))\n",
    "        print(f'Learning Rate: {learning_rate}, Batch Size: {batch_size}, 5-fold MAE: {mean_mae:.4f}')\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\nResults:\")\n",
    "for lr, bs, mae in results:\n",
    "    print(f'Learning Rate: {lr}, Batch Size: {bs}, Mean MAE: {mae:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.001, Batch Size: 16, Penalty: 0.01, Dropout: 0.3, 5-fold MAE: 12.5146\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.001, Batch Size: 16, Penalty: 0.01, Dropout: 0.5, 5-fold MAE: 12.5451\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.001, Batch Size: 16, Penalty: 0.1, Dropout: 0.3, 5-fold MAE: 12.5384\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.001, Batch Size: 16, Penalty: 0.1, Dropout: 0.5, 5-fold MAE: 12.5469\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.001, Batch Size: 16, Penalty: 0.5, Dropout: 0.3, 5-fold MAE: 12.5464\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Learning Rate: 0.001, Batch Size: 16, Penalty: 0.5, Dropout: 0.5, 5-fold MAE: 12.5517\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.001, Batch Size: 16, Penalty: 1, Dropout: 0.3, 5-fold MAE: 12.5393\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.001, Batch Size: 16, Penalty: 1, Dropout: 0.5, 5-fold MAE: 12.5486\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.001, Batch Size: 32, Penalty: 0.01, Dropout: 0.3, 5-fold MAE: 12.5617\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.001, Batch Size: 32, Penalty: 0.01, Dropout: 0.5, 5-fold MAE: 12.5420\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.001, Batch Size: 32, Penalty: 0.1, Dropout: 0.3, 5-fold MAE: 12.5380\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Learning Rate: 0.001, Batch Size: 32, Penalty: 0.1, Dropout: 0.5, 5-fold MAE: 12.5457\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.001, Batch Size: 32, Penalty: 0.5, Dropout: 0.3, 5-fold MAE: 12.5403\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.001, Batch Size: 32, Penalty: 0.5, Dropout: 0.5, 5-fold MAE: 12.5564\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.001, Batch Size: 32, Penalty: 1, Dropout: 0.3, 5-fold MAE: 12.5401\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.001, Batch Size: 32, Penalty: 1, Dropout: 0.5, 5-fold MAE: 12.5487\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Learning Rate: 0.001, Batch Size: 64, Penalty: 0.01, Dropout: 0.3, 5-fold MAE: 12.5368\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.001, Batch Size: 64, Penalty: 0.01, Dropout: 0.5, 5-fold MAE: 12.5001\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.001, Batch Size: 64, Penalty: 0.1, Dropout: 0.3, 5-fold MAE: 12.5397\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.001, Batch Size: 64, Penalty: 0.1, Dropout: 0.5, 5-fold MAE: 12.5434\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.001, Batch Size: 64, Penalty: 0.5, Dropout: 0.3, 5-fold MAE: 12.5400\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.001, Batch Size: 64, Penalty: 0.5, Dropout: 0.5, 5-fold MAE: 12.5482\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Learning Rate: 0.001, Batch Size: 64, Penalty: 1, Dropout: 0.3, 5-fold MAE: 12.5386\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Learning Rate: 0.001, Batch Size: 64, Penalty: 1, Dropout: 0.5, 5-fold MAE: 12.5453\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Learning Rate: 0.01, Batch Size: 16, Penalty: 0.01, Dropout: 0.3, 5-fold MAE: 12.5501\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.01, Batch Size: 16, Penalty: 0.01, Dropout: 0.5, 5-fold MAE: 12.5472\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.01, Batch Size: 16, Penalty: 0.1, Dropout: 0.3, 5-fold MAE: 12.5413\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.01, Batch Size: 16, Penalty: 0.1, Dropout: 0.5, 5-fold MAE: 12.5441\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Learning Rate: 0.01, Batch Size: 16, Penalty: 0.5, Dropout: 0.3, 5-fold MAE: 12.5503\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.01, Batch Size: 16, Penalty: 0.5, Dropout: 0.5, 5-fold MAE: 12.5440\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.01, Batch Size: 16, Penalty: 1, Dropout: 0.3, 5-fold MAE: 12.5542\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.01, Batch Size: 16, Penalty: 1, Dropout: 0.5, 5-fold MAE: 12.5443\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.01, Batch Size: 32, Penalty: 0.01, Dropout: 0.3, 5-fold MAE: 12.5460\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.01, Batch Size: 32, Penalty: 0.01, Dropout: 0.5, 5-fold MAE: 12.5441\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.01, Batch Size: 32, Penalty: 0.1, Dropout: 0.3, 5-fold MAE: 12.5397\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Learning Rate: 0.01, Batch Size: 32, Penalty: 0.1, Dropout: 0.5, 5-fold MAE: 12.5461\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.01, Batch Size: 32, Penalty: 0.5, Dropout: 0.3, 5-fold MAE: 12.5459\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.01, Batch Size: 32, Penalty: 0.5, Dropout: 0.5, 5-fold MAE: 12.5539\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.01, Batch Size: 32, Penalty: 1, Dropout: 0.3, 5-fold MAE: 12.5387\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.01, Batch Size: 32, Penalty: 1, Dropout: 0.5, 5-fold MAE: 12.5571\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.01, Batch Size: 64, Penalty: 0.01, Dropout: 0.3, 5-fold MAE: 12.5547\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.01, Batch Size: 64, Penalty: 0.01, Dropout: 0.5, 5-fold MAE: 12.5610\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.01, Batch Size: 64, Penalty: 0.1, Dropout: 0.3, 5-fold MAE: 12.5420\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Learning Rate: 0.01, Batch Size: 64, Penalty: 0.1, Dropout: 0.5, 5-fold MAE: 12.5478\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.01, Batch Size: 64, Penalty: 0.5, Dropout: 0.3, 5-fold MAE: 12.5461\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.01, Batch Size: 64, Penalty: 0.5, Dropout: 0.5, 5-fold MAE: 12.5588\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.01, Batch Size: 64, Penalty: 1, Dropout: 0.3, 5-fold MAE: 12.5543\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Learning Rate: 0.01, Batch Size: 64, Penalty: 1, Dropout: 0.5, 5-fold MAE: 12.5568\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.1, Batch Size: 16, Penalty: 0.01, Dropout: 0.3, 5-fold MAE: 12.5465\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.1, Batch Size: 16, Penalty: 0.01, Dropout: 0.5, 5-fold MAE: 12.5436\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.1, Batch Size: 16, Penalty: 0.1, Dropout: 0.3, 5-fold MAE: 12.6319\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Learning Rate: 0.1, Batch Size: 16, Penalty: 0.1, Dropout: 0.5, 5-fold MAE: 12.5438\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.1, Batch Size: 16, Penalty: 0.5, Dropout: 0.3, 5-fold MAE: 12.5372\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Learning Rate: 0.1, Batch Size: 16, Penalty: 0.5, Dropout: 0.5, 5-fold MAE: 12.6277\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.1, Batch Size: 16, Penalty: 1, Dropout: 0.3, 5-fold MAE: 12.5476\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.1, Batch Size: 16, Penalty: 1, Dropout: 0.5, 5-fold MAE: 12.5438\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Learning Rate: 0.1, Batch Size: 32, Penalty: 0.01, Dropout: 0.3, 5-fold MAE: 12.5490\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.1, Batch Size: 32, Penalty: 0.01, Dropout: 0.5, 5-fold MAE: 12.5377\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.1, Batch Size: 32, Penalty: 0.1, Dropout: 0.3, 5-fold MAE: 12.5644\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.1, Batch Size: 32, Penalty: 0.1, Dropout: 0.5, 5-fold MAE: 12.5439\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.1, Batch Size: 32, Penalty: 0.5, Dropout: 0.3, 5-fold MAE: 12.5371\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "Learning Rate: 0.1, Batch Size: 32, Penalty: 0.5, Dropout: 0.5, 5-fold MAE: 12.5505\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Learning Rate: 0.1, Batch Size: 32, Penalty: 1, Dropout: 0.3, 5-fold MAE: 12.5670\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.1, Batch Size: 32, Penalty: 1, Dropout: 0.5, 5-fold MAE: 12.5370\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.1, Batch Size: 64, Penalty: 0.01, Dropout: 0.3, 5-fold MAE: 12.5657\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.1, Batch Size: 64, Penalty: 0.01, Dropout: 0.5, 5-fold MAE: 12.5377\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Learning Rate: 0.1, Batch Size: 64, Penalty: 0.1, Dropout: 0.3, 5-fold MAE: 12.5880\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.1, Batch Size: 64, Penalty: 0.1, Dropout: 0.5, 5-fold MAE: 12.5386\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.1, Batch Size: 64, Penalty: 0.5, Dropout: 0.3, 5-fold MAE: 12.5655\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Learning Rate: 0.1, Batch Size: 64, Penalty: 0.5, Dropout: 0.5, 5-fold MAE: 12.5462\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.1, Batch Size: 64, Penalty: 1, Dropout: 0.3, 5-fold MAE: 12.5529\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Learning Rate: 0.1, Batch Size: 64, Penalty: 1, Dropout: 0.5, 5-fold MAE: 12.5401\n",
      "\n",
      "Results:\n",
      "Learning Rate: 0.001, Batch Size: 16, Penalty: 0.01, Dropout: 0.3, Mean MAE: 12.5146\n",
      "Learning Rate: 0.001, Batch Size: 16, Penalty: 0.01, Dropout: 0.5, Mean MAE: 12.5451\n",
      "Learning Rate: 0.001, Batch Size: 16, Penalty: 0.1, Dropout: 0.3, Mean MAE: 12.5384\n",
      "Learning Rate: 0.001, Batch Size: 16, Penalty: 0.1, Dropout: 0.5, Mean MAE: 12.5469\n",
      "Learning Rate: 0.001, Batch Size: 16, Penalty: 0.5, Dropout: 0.3, Mean MAE: 12.5464\n",
      "Learning Rate: 0.001, Batch Size: 16, Penalty: 0.5, Dropout: 0.5, Mean MAE: 12.5517\n",
      "Learning Rate: 0.001, Batch Size: 16, Penalty: 1, Dropout: 0.3, Mean MAE: 12.5393\n",
      "Learning Rate: 0.001, Batch Size: 16, Penalty: 1, Dropout: 0.5, Mean MAE: 12.5486\n",
      "Learning Rate: 0.001, Batch Size: 32, Penalty: 0.01, Dropout: 0.3, Mean MAE: 12.5617\n",
      "Learning Rate: 0.001, Batch Size: 32, Penalty: 0.01, Dropout: 0.5, Mean MAE: 12.5420\n",
      "Learning Rate: 0.001, Batch Size: 32, Penalty: 0.1, Dropout: 0.3, Mean MAE: 12.5380\n",
      "Learning Rate: 0.001, Batch Size: 32, Penalty: 0.1, Dropout: 0.5, Mean MAE: 12.5457\n",
      "Learning Rate: 0.001, Batch Size: 32, Penalty: 0.5, Dropout: 0.3, Mean MAE: 12.5403\n",
      "Learning Rate: 0.001, Batch Size: 32, Penalty: 0.5, Dropout: 0.5, Mean MAE: 12.5564\n",
      "Learning Rate: 0.001, Batch Size: 32, Penalty: 1, Dropout: 0.3, Mean MAE: 12.5401\n",
      "Learning Rate: 0.001, Batch Size: 32, Penalty: 1, Dropout: 0.5, Mean MAE: 12.5487\n",
      "Learning Rate: 0.001, Batch Size: 64, Penalty: 0.01, Dropout: 0.3, Mean MAE: 12.5368\n",
      "Learning Rate: 0.001, Batch Size: 64, Penalty: 0.01, Dropout: 0.5, Mean MAE: 12.5001\n",
      "Learning Rate: 0.001, Batch Size: 64, Penalty: 0.1, Dropout: 0.3, Mean MAE: 12.5397\n",
      "Learning Rate: 0.001, Batch Size: 64, Penalty: 0.1, Dropout: 0.5, Mean MAE: 12.5434\n",
      "Learning Rate: 0.001, Batch Size: 64, Penalty: 0.5, Dropout: 0.3, Mean MAE: 12.5400\n",
      "Learning Rate: 0.001, Batch Size: 64, Penalty: 0.5, Dropout: 0.5, Mean MAE: 12.5482\n",
      "Learning Rate: 0.001, Batch Size: 64, Penalty: 1, Dropout: 0.3, Mean MAE: 12.5386\n",
      "Learning Rate: 0.001, Batch Size: 64, Penalty: 1, Dropout: 0.5, Mean MAE: 12.5453\n",
      "Learning Rate: 0.01, Batch Size: 16, Penalty: 0.01, Dropout: 0.3, Mean MAE: 12.5501\n",
      "Learning Rate: 0.01, Batch Size: 16, Penalty: 0.01, Dropout: 0.5, Mean MAE: 12.5472\n",
      "Learning Rate: 0.01, Batch Size: 16, Penalty: 0.1, Dropout: 0.3, Mean MAE: 12.5413\n",
      "Learning Rate: 0.01, Batch Size: 16, Penalty: 0.1, Dropout: 0.5, Mean MAE: 12.5441\n",
      "Learning Rate: 0.01, Batch Size: 16, Penalty: 0.5, Dropout: 0.3, Mean MAE: 12.5503\n",
      "Learning Rate: 0.01, Batch Size: 16, Penalty: 0.5, Dropout: 0.5, Mean MAE: 12.5440\n",
      "Learning Rate: 0.01, Batch Size: 16, Penalty: 1, Dropout: 0.3, Mean MAE: 12.5542\n",
      "Learning Rate: 0.01, Batch Size: 16, Penalty: 1, Dropout: 0.5, Mean MAE: 12.5443\n",
      "Learning Rate: 0.01, Batch Size: 32, Penalty: 0.01, Dropout: 0.3, Mean MAE: 12.5460\n",
      "Learning Rate: 0.01, Batch Size: 32, Penalty: 0.01, Dropout: 0.5, Mean MAE: 12.5441\n",
      "Learning Rate: 0.01, Batch Size: 32, Penalty: 0.1, Dropout: 0.3, Mean MAE: 12.5397\n",
      "Learning Rate: 0.01, Batch Size: 32, Penalty: 0.1, Dropout: 0.5, Mean MAE: 12.5461\n",
      "Learning Rate: 0.01, Batch Size: 32, Penalty: 0.5, Dropout: 0.3, Mean MAE: 12.5459\n",
      "Learning Rate: 0.01, Batch Size: 32, Penalty: 0.5, Dropout: 0.5, Mean MAE: 12.5539\n",
      "Learning Rate: 0.01, Batch Size: 32, Penalty: 1, Dropout: 0.3, Mean MAE: 12.5387\n",
      "Learning Rate: 0.01, Batch Size: 32, Penalty: 1, Dropout: 0.5, Mean MAE: 12.5571\n",
      "Learning Rate: 0.01, Batch Size: 64, Penalty: 0.01, Dropout: 0.3, Mean MAE: 12.5547\n",
      "Learning Rate: 0.01, Batch Size: 64, Penalty: 0.01, Dropout: 0.5, Mean MAE: 12.5610\n",
      "Learning Rate: 0.01, Batch Size: 64, Penalty: 0.1, Dropout: 0.3, Mean MAE: 12.5420\n",
      "Learning Rate: 0.01, Batch Size: 64, Penalty: 0.1, Dropout: 0.5, Mean MAE: 12.5478\n",
      "Learning Rate: 0.01, Batch Size: 64, Penalty: 0.5, Dropout: 0.3, Mean MAE: 12.5461\n",
      "Learning Rate: 0.01, Batch Size: 64, Penalty: 0.5, Dropout: 0.5, Mean MAE: 12.5588\n",
      "Learning Rate: 0.01, Batch Size: 64, Penalty: 1, Dropout: 0.3, Mean MAE: 12.5543\n",
      "Learning Rate: 0.01, Batch Size: 64, Penalty: 1, Dropout: 0.5, Mean MAE: 12.5568\n",
      "Learning Rate: 0.1, Batch Size: 16, Penalty: 0.01, Dropout: 0.3, Mean MAE: 12.5465\n",
      "Learning Rate: 0.1, Batch Size: 16, Penalty: 0.01, Dropout: 0.5, Mean MAE: 12.5436\n",
      "Learning Rate: 0.1, Batch Size: 16, Penalty: 0.1, Dropout: 0.3, Mean MAE: 12.6319\n",
      "Learning Rate: 0.1, Batch Size: 16, Penalty: 0.1, Dropout: 0.5, Mean MAE: 12.5438\n",
      "Learning Rate: 0.1, Batch Size: 16, Penalty: 0.5, Dropout: 0.3, Mean MAE: 12.5372\n",
      "Learning Rate: 0.1, Batch Size: 16, Penalty: 0.5, Dropout: 0.5, Mean MAE: 12.6277\n",
      "Learning Rate: 0.1, Batch Size: 16, Penalty: 1, Dropout: 0.3, Mean MAE: 12.5476\n",
      "Learning Rate: 0.1, Batch Size: 16, Penalty: 1, Dropout: 0.5, Mean MAE: 12.5438\n",
      "Learning Rate: 0.1, Batch Size: 32, Penalty: 0.01, Dropout: 0.3, Mean MAE: 12.5490\n",
      "Learning Rate: 0.1, Batch Size: 32, Penalty: 0.01, Dropout: 0.5, Mean MAE: 12.5377\n",
      "Learning Rate: 0.1, Batch Size: 32, Penalty: 0.1, Dropout: 0.3, Mean MAE: 12.5644\n",
      "Learning Rate: 0.1, Batch Size: 32, Penalty: 0.1, Dropout: 0.5, Mean MAE: 12.5439\n",
      "Learning Rate: 0.1, Batch Size: 32, Penalty: 0.5, Dropout: 0.3, Mean MAE: 12.5371\n",
      "Learning Rate: 0.1, Batch Size: 32, Penalty: 0.5, Dropout: 0.5, Mean MAE: 12.5505\n",
      "Learning Rate: 0.1, Batch Size: 32, Penalty: 1, Dropout: 0.3, Mean MAE: 12.5670\n",
      "Learning Rate: 0.1, Batch Size: 32, Penalty: 1, Dropout: 0.5, Mean MAE: 12.5370\n",
      "Learning Rate: 0.1, Batch Size: 64, Penalty: 0.01, Dropout: 0.3, Mean MAE: 12.5657\n",
      "Learning Rate: 0.1, Batch Size: 64, Penalty: 0.01, Dropout: 0.5, Mean MAE: 12.5377\n",
      "Learning Rate: 0.1, Batch Size: 64, Penalty: 0.1, Dropout: 0.3, Mean MAE: 12.5880\n",
      "Learning Rate: 0.1, Batch Size: 64, Penalty: 0.1, Dropout: 0.5, Mean MAE: 12.5386\n",
      "Learning Rate: 0.1, Batch Size: 64, Penalty: 0.5, Dropout: 0.3, Mean MAE: 12.5655\n",
      "Learning Rate: 0.1, Batch Size: 64, Penalty: 0.5, Dropout: 0.5, Mean MAE: 12.5462\n",
      "Learning Rate: 0.1, Batch Size: 64, Penalty: 1, Dropout: 0.3, Mean MAE: 12.5529\n",
      "Learning Rate: 0.1, Batch Size: 64, Penalty: 1, Dropout: 0.5, Mean MAE: 12.5401\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 데이터 로드\n",
    "train = pd.read_csv('./train.csv')\n",
    "\n",
    "# yymm 컬럼을 날짜 형식으로 변환 (연도는 임의로 설정)\n",
    "train['yymm'] = pd.to_datetime('2024' + train['yymm'], format='%Y%m%d %H:%M')\n",
    "\n",
    "# day, hour, minute 컬럼 생성\n",
    "train['day'] = train['yymm'].dt.day\n",
    "train['hour'] = train['yymm'].dt.hour\n",
    "train['minute'] = train['yymm'].dt.minute\n",
    "\n",
    "# weekday 컬럼 생성\n",
    "train['weekday'] = train['day'] % 7\n",
    "\n",
    "# yymm 컬럼 삭제\n",
    "train.drop('yymm', axis=1, inplace=True)\n",
    "\n",
    "# 데이터 분할\n",
    "X = train.drop('Target', axis=1)\n",
    "y = train['Target']\n",
    "\n",
    "# 특징 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# K-fold cross-validation 설정 (5-fold)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 설정한 하이퍼파라미터\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "batch_sizes = [16, 32, 64]\n",
    "penalties = [0.01, 0.1, 0.5, 1]\n",
    "drop_outs = [0.3, 0.5]\n",
    "\n",
    "# 성능 비교를 위한 결과 저장\n",
    "results = []\n",
    "\n",
    "# 딥러닝 회귀 모델 정의 함수 (DNN)\n",
    "def create_model(learning_rate, penalty, drop_out):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_scaled.shape[1],)),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l1(penalty)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(drop_out),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1(penalty)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(drop_out),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l1(penalty)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(drop_out),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
    "    return model\n",
    "\n",
    "# 다양한 learning rate, batch size, penalty, dropout에 대해 성능 비교\n",
    "for learning_rate in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        for penalty in penalties:\n",
    "            for drop_out in drop_outs:\n",
    "                mae_scores = []\n",
    "\n",
    "                # 5-fold 교차 검증\n",
    "                for train_index, val_index in kf.split(X_scaled):\n",
    "                    X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "                    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "                    # 모델 생성 및 학습\n",
    "                    model = create_model(float(learning_rate), float(penalty), float(drop_out))\n",
    "\n",
    "\n",
    "                    # EarlyStopping 설정\n",
    "                    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "                    model.fit(X_train, y_train, epochs=100, batch_size=batch_size, validation_data=(X_val, y_val),\n",
    "                              verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "                    # 예측 및 MAE 계산\n",
    "                    y_pred = model.predict(X_val)\n",
    "                    mae = mean_absolute_error(y_val, y_pred)\n",
    "                    mae_scores.append(mae)\n",
    "\n",
    "                # 평균 MAE 결과 저장\n",
    "                mean_mae = np.mean(mae_scores)\n",
    "                results.append((learning_rate, batch_size, penalty, drop_out, mean_mae))\n",
    "                print(f'Learning Rate: {learning_rate}, Batch Size: {batch_size}, Penalty: {penalty}, '\n",
    "                      f'Dropout: {drop_out}, 5-fold MAE: {mean_mae:.4f}')\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\nResults:\")\n",
    "for lr, bs, pen, drp, mae in results:\n",
    "    print(f'Learning Rate: {lr}, Batch Size: {bs}, Penalty: {pen}, Dropout: {drp}, Mean MAE: {mae:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Learning Rate: 0.001, Batch Size: 16, 5-fold MAE: 12.5226\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Learning Rate: 0.001, Batch Size: 32, 5-fold MAE: 12.5216\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.001, Batch Size: 64, 5-fold MAE: 12.5062\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Learning Rate: 0.01, Batch Size: 16, 5-fold MAE: 12.5503\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Learning Rate: 0.01, Batch Size: 32, 5-fold MAE: 12.5409\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Learning Rate: 0.01, Batch Size: 64, 5-fold MAE: 12.5465\n",
      "\n",
      "Results:\n",
      "Learning Rate: 0.001, Batch Size: 16, Mean MAE: 12.5226\n",
      "Learning Rate: 0.001, Batch Size: 32, Mean MAE: 12.5216\n",
      "Learning Rate: 0.001, Batch Size: 64, Mean MAE: 12.5062\n",
      "Learning Rate: 0.01, Batch Size: 16, Mean MAE: 12.5503\n",
      "Learning Rate: 0.01, Batch Size: 32, Mean MAE: 12.5409\n",
      "Learning Rate: 0.01, Batch Size: 64, Mean MAE: 12.5465\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1  # L1 정규화 임포트\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "np.random.seed(42)  # NumPy 랜덤 시드\n",
    "tf.random.set_seed(42)  # TensorFlow 랜덤 시드\n",
    "\n",
    "# 데이터 로드\n",
    "train = pd.read_csv('./train.csv')\n",
    "\n",
    "# yymm 컬럼을 날짜 형식으로 변환 (연도는 임의로 설정)\n",
    "train['yymm'] = pd.to_datetime('2024' + train['yymm'], format='%Y%m%d %H:%M')\n",
    "\n",
    "# day, hour, minute 컬럼 생성\n",
    "train['day'] = train['yymm'].dt.day         # 일\n",
    "train['hour'] = train['yymm'].dt.hour       # 시\n",
    "train['minute'] = train['yymm'].dt.minute   # 분\n",
    "\n",
    "# weekday 컬럼 생성\n",
    "train['weekday'] = train['day'] % 7         # 요일 (0: 월요일, 1: 화요일, ..., 6: 일요일)\n",
    "\n",
    "# yymm 컬럼 삭제\n",
    "train.drop('yymm', axis=1, inplace=True)\n",
    "\n",
    "# 데이터 분할\n",
    "columns = ['V2', 'V7', 'V11', 'V14', 'V20', 'V21', 'Target']\n",
    "X = train.drop(columns = columns)    # Target을 제외한 모든 컬럼을 X로 지정\n",
    "y = train['Target']                 # Target 컬럼을 y로 지정\n",
    "\n",
    "# 특징 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# K-fold cross-validation 설정 (5-fold)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 설정한 하이퍼파라미터\n",
    "learning_rates = [0.001, 0.01]  # 여러 learning rate\n",
    "batch_sizes = [16, 32, 64]       # 여러 batch size\n",
    "\n",
    "# 성능 비교를 위한 결과 저장\n",
    "results = []\n",
    "\n",
    "# 딥러닝 회귀 모델 정의 함수 (DNN)\n",
    "def create_model(learning_rate):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_scaled.shape[1],)),  # Input layer로 input shape 지정\n",
    "        Dense(128, activation='relu', kernel_regularizer=l1(0.01)),  # DNN 레이어\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1(0.01)),   # DNN 레이어\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l1(0.01)),   # DNN 레이어\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1)  # 회귀 모델이므로 활성화 함수 없이 출력\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
    "    return model\n",
    "\n",
    "# 다양한 learning rate와 batch size에 대해 성능 비교\n",
    "for learning_rate in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        mae_scores = []\n",
    "\n",
    "        # 5-fold 교차 검증\n",
    "        for train_index, val_index in kf.split(X_scaled):\n",
    "            X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "            # 모델 생성 및 학습\n",
    "            model = create_model(learning_rate)\n",
    "\n",
    "            # EarlyStopping 설정\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "            model.fit(X_train, y_train, epochs=100, batch_size=batch_size, validation_data=(X_val, y_val),verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "            # 예측 및 MAE 계산\n",
    "            y_pred = model.predict(X_val)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            mae_scores.append(mae)\n",
    "\n",
    "        # 평균 MAE 결과 저장\n",
    "        mean_mae = np.mean(mae_scores)\n",
    "        results.append((learning_rate, batch_size, mean_mae))\n",
    "        print(f'Learning Rate: {learning_rate}, Batch Size: {batch_size}, 5-fold MAE: {mean_mae:.4f}')\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\nResults:\")\n",
    "for lr, bs, mae in results:\n",
    "    print(f'Learning Rate: {lr}, Batch Size: {bs}, Mean MAE: {mae:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 28.7296\n",
      "Epoch 2/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 17.9779\n",
      "Epoch 3/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 16.4685\n",
      "Epoch 4/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 16.0187\n",
      "Epoch 5/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 15.0634\n",
      "Epoch 6/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 14.4083\n",
      "Epoch 7/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 14.2020\n",
      "Epoch 8/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 13.8331\n",
      "Epoch 9/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.7286\n",
      "Epoch 10/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 13.4055\n",
      "Epoch 11/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.3337\n",
      "Epoch 12/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.1867\n",
      "Epoch 13/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.2507\n",
      "Epoch 14/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.2120\n",
      "Epoch 15/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.3202\n",
      "Epoch 16/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 13.2152\n",
      "Epoch 17/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.2815\n",
      "Epoch 18/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.1178\n",
      "Epoch 19/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.2544\n",
      "Epoch 20/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.3100\n",
      "Epoch 21/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 13.2078\n",
      "Epoch 22/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.3372\n",
      "Epoch 23/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 13.2248\n",
      "Epoch 24/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 13.1877\n",
      "Epoch 25/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 12.9884\n",
      "Epoch 26/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 13.0379\n",
      "Epoch 27/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 12.9996\n",
      "Epoch 28/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 13.0991\n",
      "Epoch 29/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 13.1397\n",
      "Epoch 30/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 13.0069\n",
      "Epoch 31/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 13.2379\n",
      "Epoch 32/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 13.0861\n",
      "Epoch 33/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 13.0368\n",
      "Epoch 34/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 13.2106\n",
      "Epoch 35/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 13.0996\n",
      "Epoch 36/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 13.1223\n",
      "Epoch 37/100\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 13.1622\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "     predict\n",
      "0  24.274851\n",
      "1  24.595993\n",
      "2  24.448538\n",
      "3  24.296534\n",
      "4  25.003271\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1  # L1 regularization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "# Fix random seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test_set.csv')\n",
    "\n",
    "# Convert 'yymm' to datetime format (adding an arbitrary year)\n",
    "train['yymm'] = pd.to_datetime('2024' + train['yymm'], format='%Y%m%d %H:%M')\n",
    "test['yymm'] = pd.to_datetime('2024' + test['yymm'], format='%Y%m%d %H:%M')\n",
    "\n",
    "# Extract day, hour, minute features\n",
    "train['day'] = train['yymm'].dt.day\n",
    "train['hour'] = train['yymm'].dt.hour\n",
    "train['minute'] = train['yymm'].dt.minute\n",
    "\n",
    "test['day'] = test['yymm'].dt.day\n",
    "test['hour'] = test['yymm'].dt.hour\n",
    "test['minute'] = test['yymm'].dt.minute\n",
    "\n",
    "# Generate weekday feature\n",
    "train['weekday'] = train['day'] % 7\n",
    "test['weekday'] = test['day'] % 7\n",
    "\n",
    "# Drop 'yymm' column\n",
    "train.drop('yymm', axis=1, inplace=True)\n",
    "test.drop('yymm', axis=1, inplace=True)\n",
    "\n",
    "# Split into features and target\n",
    "columns = ['']\n",
    "X = train.drop('Target', axis=1)\n",
    "y = train['Target']\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(test)\n",
    "\n",
    "# Define the DNN model for regression\n",
    "def create_model(learning_rate):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_scaled.shape[1],)),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l1(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l1(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1)  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
    "    return model\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "\n",
    "# Create the model\n",
    "model = create_model(learning_rate)\n",
    "\n",
    "# Set up EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model on the entire dataset without validation split\n",
    "model.fit(X_scaled, y, epochs=100, batch_size=batch_size, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Generate submission file\n",
    "submission = pd.DataFrame(test_pred, columns=['predict'])\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - loss: 31.1712\n",
      "Epoch 2/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 18.1303\n",
      "Epoch 3/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 16.5869\n",
      "Epoch 4/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 16.2886\n",
      "Epoch 5/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 15.5712\n",
      "Epoch 6/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 15.1483\n",
      "Epoch 7/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.9482\n",
      "Epoch 8/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14.4562\n",
      "Epoch 9/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 14.3669\n",
      "Epoch 10/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 13.9408\n",
      "Epoch 11/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.7222\n",
      "Epoch 12/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.5494\n",
      "Epoch 13/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 13.5810\n",
      "Epoch 14/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.6811\n",
      "Epoch 15/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 13.3966\n",
      "Epoch 16/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 13.2861\n",
      "Epoch 17/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.2881\n",
      "Epoch 18/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.2052\n",
      "Epoch 19/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.3249\n",
      "Epoch 20/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.3395\n",
      "Epoch 21/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 13.2017\n",
      "Epoch 22/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 13.2264\n",
      "Epoch 23/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 13.1042\n",
      "Epoch 24/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 13.2095\n",
      "Epoch 25/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 13.2190\n",
      "Epoch 26/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 13.0749\n",
      "Epoch 27/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.1139\n",
      "Epoch 28/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.1103\n",
      "Epoch 29/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.1664\n",
      "Epoch 30/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.0756\n",
      "Epoch 31/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.1119\n",
      "Epoch 32/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.0644\n",
      "Epoch 33/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 13.0082\n",
      "Epoch 34/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.0573\n",
      "Epoch 35/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.0446\n",
      "Epoch 36/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.0895\n",
      "Epoch 37/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 13.0590\n",
      "Epoch 38/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.0421\n",
      "Epoch 39/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.1818\n",
      "Epoch 40/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.1191\n",
      "Epoch 41/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 13.1303\n",
      "Epoch 42/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 13.0272\n",
      "Epoch 43/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.1210\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "     predict\n",
      "0  24.265726\n",
      "1  24.735556\n",
      "2  24.454250\n",
      "3  24.332586\n",
      "4  25.088556\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1  # L1 regularization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "# Fix random seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test_set.csv')\n",
    "\n",
    "# Convert 'yymm' to datetime format (adding an arbitrary year)\n",
    "train['yymm'] = pd.to_datetime('2024' + train['yymm'], format='%Y%m%d %H:%M')\n",
    "test['yymm'] = pd.to_datetime('2024' + test['yymm'], format='%Y%m%d %H:%M')\n",
    "\n",
    "# Extract day, hour, minute features\n",
    "train['day'] = train['yymm'].dt.day\n",
    "train['hour'] = train['yymm'].dt.hour\n",
    "train['minute'] = train['yymm'].dt.minute\n",
    "\n",
    "test['day'] = test['yymm'].dt.day\n",
    "test['hour'] = test['yymm'].dt.hour\n",
    "test['minute'] = test['yymm'].dt.minute\n",
    "\n",
    "# Generate weekday feature\n",
    "train['weekday'] = train['day'] % 7\n",
    "test['weekday'] = test['day'] % 7\n",
    "\n",
    "# Drop 'yymm' column\n",
    "train.drop('yymm', axis=1, inplace=True)\n",
    "test.drop('yymm', axis=1, inplace=True)\n",
    "\n",
    "# Split into features and target\n",
    "X = train.drop('Target', axis=1)\n",
    "y = train['Target']\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(test)\n",
    "\n",
    "# Define the DNN model for regression\n",
    "def create_model(learning_rate):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_scaled.shape[1],)),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l1(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l1(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1)  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
    "    return model\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "\n",
    "# Create the model\n",
    "model = create_model(learning_rate)\n",
    "\n",
    "# Set up EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model on the entire dataset without validation split\n",
    "model.fit(X_scaled, y, epochs=100, batch_size=batch_size, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Generate submission file\n",
    "submission = pd.DataFrame(test_pred, columns=['predict'])\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 30.0766\n",
      "Epoch 2/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 17.0913\n",
      "Epoch 3/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.9871\n",
      "Epoch 4/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 16.0179\n",
      "Epoch 5/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.5100\n",
      "Epoch 6/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.1026\n",
      "Epoch 7/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.0664\n",
      "Epoch 8/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.5273\n",
      "Epoch 9/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.4480\n",
      "Epoch 10/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.1738\n",
      "Epoch 11/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.9921\n",
      "Epoch 12/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.8135\n",
      "Epoch 13/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.7390\n",
      "Epoch 14/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.5493\n",
      "Epoch 15/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.4581\n",
      "Epoch 16/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.4363\n",
      "Epoch 17/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.2907\n",
      "Epoch 18/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.3640\n",
      "Epoch 19/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.3321\n",
      "Epoch 20/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.2196\n",
      "Epoch 21/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.2417\n",
      "Epoch 22/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.1601\n",
      "Epoch 23/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.2499\n",
      "Epoch 24/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.1554\n",
      "Epoch 25/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.1625\n",
      "Epoch 26/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.1579\n",
      "Epoch 27/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.2709\n",
      "Epoch 28/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 13.2034\n",
      "Epoch 29/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.2042\n",
      "Epoch 30/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 13.1368\n",
      "Epoch 31/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 13.1977\n",
      "Epoch 32/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 13.1393\n",
      "Epoch 33/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.2616\n",
      "Epoch 34/100\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 13.3627\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "     predict\n",
      "0  23.615725\n",
      "1  23.638554\n",
      "2  23.641716\n",
      "3  23.613964\n",
      "4  23.636137\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1  # L1 regularization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "# Fix random seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test_set.csv')\n",
    "\n",
    "# Convert 'yymm' to datetime format (adding an arbitrary year)\n",
    "train['yymm'] = pd.to_datetime('2024' + train['yymm'], format='%Y%m%d %H:%M')\n",
    "test['yymm'] = pd.to_datetime('2024' + test['yymm'], format='%Y%m%d %H:%M')\n",
    "\n",
    "# Extract day, hour, minute features\n",
    "train['day'] = train['yymm'].dt.day\n",
    "train['hour'] = train['yymm'].dt.hour\n",
    "train['minute'] = train['yymm'].dt.minute\n",
    "\n",
    "test['day'] = test['yymm'].dt.day\n",
    "test['hour'] = test['yymm'].dt.hour\n",
    "test['minute'] = test['yymm'].dt.minute\n",
    "\n",
    "# Generate weekday feature\n",
    "train['weekday'] = train['day'] % 7\n",
    "test['weekday'] = test['day'] % 7\n",
    "\n",
    "# Drop 'yymm' column\n",
    "train.drop('yymm', axis=1, inplace=True)\n",
    "test.drop('yymm', axis=1, inplace=True)\n",
    "\n",
    "# Split into features and target\n",
    "X = train.drop('Target', axis=1)\n",
    "y = train['Target']\n",
    "\n",
    "# Define the DNN model for regression\n",
    "def create_model(learning_rate):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_scaled.shape[1],)),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l1(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l1(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1)  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
    "    return model\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "\n",
    "# Create the model\n",
    "model = create_model(learning_rate)\n",
    "\n",
    "# Set up EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model on the entire dataset without validation split\n",
    "model.fit(X, y, epochs=100, batch_size=batch_size, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_pred = model.predict(test)\n",
    "\n",
    "# Generate submission file\n",
    "submission = pd.DataFrame(test_pred, columns=['predict'])\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(submission.head())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
